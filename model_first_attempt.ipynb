{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.ops import nms\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "WANDB_LOGGING = False\n",
    "FREEZE_FEATURE_EXTRACTOR = True\n",
    "CONFIG = {\n",
    "    \"project_name\": \"name\",\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"bias\": True,\n",
    "    \"lr\": 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(box1_cx, box1_cy, box1_w, box1_h, box2_cx, box2_cy, box2_w, box2_h):\n",
    "    area1 = box1_w * box1_h\n",
    "    area2 = box2_w * box2_h\n",
    "\n",
    "    top_left1 = (box1_cx - (box1_w/2), box1_cy - (box1_h/2))\n",
    "    top_left2 = (box2_cx - (box2_w/2), box2_cy - (box2_h/2))\n",
    "\n",
    "    bottom_right1 = (box1_cx + (box1_w/2), box1_cy + (box1_h/2))\n",
    "    bottom_right2 = (box2_cx + (box2_w/2), box2_cy + (box2_h/2))\n",
    "\n",
    "    xx = max(top_left1[0], top_left2[0])\n",
    "    yy = max(top_left1[1], top_left2[1])\n",
    "    aa = min(bottom_right1[0], bottom_right2[0])\n",
    "    bb = min(bottom_right1[1], bottom_right2[1])\n",
    "\n",
    "    w = max(0, aa - xx)\n",
    "    h = max(0, bb - yy)\n",
    "\n",
    "    intersection_area = w*h\n",
    "    union_area = area1 + area2 - intersection_area\n",
    "\n",
    "    print(f\"top_left1:{top_left1}, bottom_right1:{bottom_right1}, top_left2:{top_left2}, bottom_right2:{bottom_right2}\")\n",
    "\n",
    "    return intersection_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_left1:(1.5, 2.0), bottom_right1:(4.5, 4.0), top_left2:(3.5, 3.5), bottom_right2:(4.5, 4.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box1 = (3,3, 3, 2)\n",
    "box2 = (4,4, 1, 1)\n",
    "intersection_over_union(box1[0],box1[1],box1[2],box1[3], box2[0], box2[1], box2[2], box2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDetector(nn.Module):\n",
    "    def __init__(self, num_cells, num_anchors):\n",
    "        super(CardDetector, self).__init__()\n",
    "\n",
    "        self.num_cells = num_cells\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])\n",
    "        if FREEZE_FEATURE_EXTRACTOR:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.detection_head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, self.num_cells * self.num_anchors * 5, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.feature_extractor(input)\n",
    "        print(features.shape)\n",
    "\n",
    "        detection = self.detection_head(features)\n",
    "        print(detection.shape)\n",
    "        detection = detection.permute(0, 2, 3, 1)\n",
    "        print(detection.shape)\n",
    "        detection = detection.view(-1, self.num_cells * self.num_anchors, 5)\n",
    "        print(detection.shape)\n",
    "        detection[:, :, 0] = torch.sigmoid(detection[:, 0, :1])\n",
    "        \n",
    "        return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDetectorMultiBox(nn.Module):\n",
    "    def __init__(self, num_anchors, num_cells, max_boxes=5):\n",
    "        super(CardDetectorMultiBox, self).__init__()\n",
    "\n",
    "        self.num_anchors = num_anchors\n",
    "        self.num_cells = num_cells\n",
    "        self.max_boxes = max_boxes\n",
    "        \n",
    "        self.detector = CardDetector(num_anchors=num_anchors, num_cells=num_cells)\n",
    "\n",
    "        self.box_regression_head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, max_boxes * 4, kernel_size=1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(max_boxes, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, max_boxes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        features = self.detector.feature_extractor(input)\n",
    "\n",
    "        detection = self.detector(input)\n",
    "        box_regression = self.box_regression_head(features)\n",
    "\n",
    "        #print(f\"detection:{detection.shape}, box_reg:{box_regression.shape}\")\n",
    "\n",
    "        attention_weights = self.attention(detection)\n",
    "        #print(attention_weights)\n",
    "        #num_boxes_float = torch.sum(attention_weights, dim=1)\n",
    "        #print(num_boxes_float)\n",
    "        #num_boxes_float = torch.clamp(num_boxes_float, min=1)\n",
    "        #print(num_boxes_float)\n",
    "        #num_boxes_float = num_boxes_float / torch.sum(num_boxes_float, dim=0, keepdim=True)\n",
    "        #print(num_boxes_float)\n",
    "        #num_boxes = (num_boxes_float * self.max_boxes).round().to(torch.int)\n",
    "\n",
    "        #print(f\"num_boxes:{num_boxes}\")\n",
    "\n",
    "        print(f\"detection:{detection.shape}, box_reg:{box_regression.shape}, attention_weights:{attention_weights.shape}\") #, num_boxes:{num_boxes.shape}\")\n",
    "\n",
    "        #detection_scores = detection[:, :, :1]\n",
    "        #_, topk_indices = torch.topk(detection_scores, k=self.max_boxes, dim=1)\n",
    "        #detection_topk = torch.gather(detection, dim=1, index=topk_indices)\n",
    "#\n",
    "        #print(f\"detection_scores:{detection_scores.shape}, topk_indices:{topk_indices.shape}, detection_topk:{detection_topk.shape}\")\n",
    "        #print(f\"box_regression:{box_regression.shape}\")\n",
    "\n",
    "        #box_regression_topk = torch.gather(box_regression, dim=1, index=topk_indices)\n",
    "        \n",
    "        boxes = detection[:,0,1:]\n",
    "        scores = detection[:,0, :1]\n",
    "        print(boxes.shape)\n",
    "\n",
    "        \n",
    "        box_indices = nms(boxes=boxes, scores=scores, iou_threshold=0.2)\n",
    "        print(box_indices)\n",
    "        detection = detection[box_indices]\n",
    "\n",
    "        return detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # We use tqdm to display a simple progress bar, allowing us to observe the learning progression.\n",
    "\n",
    "def fit(\n",
    "  model: nn.Module,\n",
    "  num_epochs: int,\n",
    "  optimizer: torch.optim.Optimizer,\n",
    "  train_dataloader: DataLoader,\n",
    "  val_dataloader: DataLoader,\n",
    "  print_rate: int = 100\n",
    "  ):\n",
    "    # TODO: figure out accuacy\n",
    "    #accuracy = torchmetrics.Accuracy(task='multiclass', average=\"weighted\").to(model.device)\n",
    "    accuracy = None\n",
    "    model = model.to(model.device)\n",
    "    # Iterate through epochs with tqdm\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch: {epoch}\\n\")\n",
    "        train_loss = 0\n",
    "        model.train()  # Set mode of model to train\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            loss = model.train_step(X, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Getting the loss gradient and making an optimizer step\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % print_rate == 0: \n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{batch * len(X)}/{len(train_dataloader.dataset)} Samples\")\n",
    "            elif batch == len(train_dataloader) - 1:\n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{len(train_dataloader.dataset)}/{len(train_dataloader.dataset)} Samples\")\n",
    "        \n",
    "        # Divide the train_loss by the number of batches to get the average train_loss\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        # Setup the Val Loss and Accuracy to accumulate over the batches in the val dataset\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        # Set model to evaluation mode and use torch.inference_mode to remove unnecessary training operations \n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X_val, y_val in val_dataloader:\n",
    "                loss, acc = model.val_step(X_val, y_val, accuracy)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += acc\n",
    "\n",
    "        # Get the average Val Loss and Accuracy\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        avg_val_acc = val_acc / len(val_dataloader)\n",
    "\n",
    "        print(f\"Train loss: {avg_train_loss} | Val Loss: {avg_val_loss} | Val Accuracy: {avg_val_acc}\")\n",
    "        if WANDB_LOGGING:\n",
    "            wandb.log({\"Train Loss\": avg_train_loss,\"Val Loss\": avg_val_loss, \"Val Accuracy\": avg_val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 1, 1])\n",
      "torch.Size([2, 80, 1, 1])\n",
      "torch.Size([2, 1, 1, 80])\n",
      "torch.Size([2, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "model = CardDetector(num_cells=4, num_anchors=4)\n",
    "model.eval()\n",
    "\n",
    "output_tensor = model(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
