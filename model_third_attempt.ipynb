{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision.ops import nms, box_convert\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "WANDB_LOGGING = False\n",
    "FREEZE_FEATURE_EXTRACTOR = False\n",
    "CONFIG = {\n",
    "    \"project_name\": \"card detector\",\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 0.0001,\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"img_dir\": \"data/images/\",\n",
    "        \"annotations_file\": \"data/labels.csv\",\n",
    "        \"img_w\": 640,\n",
    "        \"img_h\": 640,\n",
    "        \"num_anchors_per_cell\": 3,\n",
    "        \"anchor_box_sizes\": [8192],\n",
    "        \"anchor_box_aspect_ratios\": [0.75, 1, 1.25],\n",
    "\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB_LOGGING: \n",
    "    wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_anchors_per_cell: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        out_channels = num_anchors_per_cell * 5\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1)\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDetector(nn.Module):\n",
    "    def __init__(self, img_dims, anchor_boxes: torch.Tensor, num_anchors_per_cell: int, loss_fn, num_max_boxes: int = 1):\n",
    "        super(CardDetector, self).__init__()\n",
    "\n",
    "        self.img_w = img_dims[0]\n",
    "        self.img_h = img_dims[1]\n",
    "        self.anchor_boxes = anchor_boxes\n",
    "        self.num_anchors_per_cell = num_anchors_per_cell\n",
    "        self.num_max_boxes = num_max_boxes\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-2])\n",
    "        if FREEZE_FEATURE_EXTRACTOR:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.detection_head = DetectionHead(in_channels=512, num_anchors_per_cell=self.num_anchors_per_cell)\n",
    "\n",
    "        dummy_image = torch.randn(1, 3, self.img_w, self.img_h)\n",
    "        features_shape = self.feature_extractor(dummy_image).shape\n",
    "        self.features_w = features_shape[-2]\n",
    "        self.features_h = features_shape[-1]\n",
    "        self.scale_w = self.img_w / self.features_w\n",
    "        self.scale_h = self.img_h / self.features_h\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Get feature map\n",
    "        features = self.feature_extractor(input)\n",
    "\n",
    "        # Get detection vectors for each feature\n",
    "        detection = self.detection_head(features)\n",
    "        detection = detection.permute(0,2,3,1).contiguous()\n",
    "        detection = detection.view(detection.shape[0], detection.shape[1], detection.shape[2], self.num_anchors_per_cell, 5)\n",
    "\n",
    "        # Apply sigmoid to the first 3 elements of the (p, cx, cy, w, h) tensor\n",
    "        #detection[:, :, :, :, :3] = torch.sigmoid(detection[:, :, :, :, :3])  \n",
    "\n",
    "        # Square the last two numbers (scales of width and height)\n",
    "        #detection[:, :, :, :, 3:] = torch.square(detection[:, :, :, :, 3:])\n",
    "        \n",
    "        return detection\n",
    "\n",
    "    def predict(self, input, ground_truth=None):\n",
    "        self.eval()\n",
    "\n",
    "        if (len(input.shape) == 3): # If we get a single image with shape (C x W x H) we need to add a dimension at the beginning so that the forward function can process it (only works on batched input)\n",
    "            input = input.unsqueeze(0) \n",
    "\n",
    "        detection = self.forward(input)\n",
    "        if ground_truth != None:\n",
    "            detection = ground_truth\n",
    "\n",
    "        anchor_box_scales = self.create_anchor_box_scales(detection_shape=detection.shape)\n",
    "        anchor_box_offsets = self.create_anchor_box_offsets(detection_shape=detection.shape, scale_w=self.scale_w, scale_h=self.scale_h)\n",
    "\n",
    "        #anchor_box_scales = anchor_box_scales.to(self.device)\n",
    "        #anchor_box_offsets = anchor_box_offsets.to(self.device)\n",
    "\n",
    "        detection[:,:,:,:,3:5] = torch.mul(detection[:,:,:,:,3:5], anchor_box_scales[:,:,:,:,3:5]) # multiply the w, h coords of detection with predifined anchor box w, h\n",
    "        detection[:,:,:,:,1] = torch.mul(detection[:,:,:,:,1], self.scale_w)   # scale the x offset from cell orgin\n",
    "        detection[:,:,:,:,2] = torch.mul(detection[:,:,:,:,2], self.scale_h)   # scale the y offset from cell origin\n",
    "        detection[:,:,:,:,1:3] = torch.add(detection[:,:,:,:,1:3], anchor_box_offsets[:,:,:,:,1:3])   # add offset from image origin\n",
    "\n",
    "        # Apply sigmoid to the objectness scores\n",
    "        detection[:,:,:,:,0] = torch.sigmoid(detection[:,:,:,:,0])\n",
    "\n",
    "        wh_offsets = detection[:, :, :, :, 3:5].clone()\n",
    "        wh_offsets = torch.mul(wh_offsets, 0.5)\n",
    "\n",
    "        cx_cy = detection[:, :, :, :, 1:3].clone()\n",
    "\n",
    "        pred_boxes = detection.clone()\n",
    "        pred_boxes[:, :, :, :, 1:3] = torch.add(cx_cy, -1 * wh_offsets)\n",
    "        pred_boxes[:, :, :, :, 3:5] = torch.add(cx_cy, wh_offsets)\n",
    "        \n",
    "        pred_boxes = pred_boxes.view(-1, pred_boxes.shape[1] * pred_boxes.shape[2] * self.num_anchors_per_cell, 5)\n",
    "\n",
    "        for image in pred_boxes:\n",
    "            boxes =  image[:, 1:]  # select the coordinate values\n",
    "            objectness_scores = image[:, :1].squeeze(dim=1) # select the objectness score values, the squeeze to get rid of the extra dimension\n",
    "\n",
    "            indices_to_keep = nms(boxes=boxes, scores=objectness_scores, iou_threshold=0.5)\n",
    "            kept_boxes = boxes[indices_to_keep[:self.num_max_boxes]]\n",
    "            kept_obj_scores = objectness_scores[indices_to_keep[:self.num_max_boxes]]\n",
    "\n",
    "            print(kept_boxes, kept_obj_scores)\n",
    "            return kept_boxes\n",
    "        \n",
    "    def create_anchor_box_offsets(self, detection_shape, scale_w, scale_h):\n",
    "        addition_tensor = torch.zeros(detection_shape[0], detection_shape[1], detection_shape[2], detection_shape[3], detection_shape[4])\n",
    "        for i in range(detection_shape[1]):\n",
    "            for j in range(detection_shape[2]):\n",
    "                addition_tensor[:, i, j, :, 1] = i * scale_w\n",
    "                addition_tensor[:, i, j, :, 2] = j * scale_h      \n",
    "        return addition_tensor\n",
    "\n",
    "    def create_anchor_box_scales(self, detection_shape):\n",
    "        tensor = torch.zeros(detection_shape[0], detection_shape[1], detection_shape[2], detection_shape[3], detection_shape[4])\n",
    "\n",
    "        for k in range(detection_shape[3]): # num of anchors\n",
    "            tensor[:, :, :, k, 3] = self.anchor_boxes[k][0]\n",
    "            tensor[:, :, :, k, 4] = self.anchor_boxes[k][1]\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_boxes(sizes, aspect_ratios) -> torch.Tensor:    #TODO: rework how we generate anchor boxes - K-means from dataset, probably\n",
    "    anchor_boxes = []\n",
    "    for size in sizes:\n",
    "        for aspect_ratio in aspect_ratios:\n",
    "            width = int((size * aspect_ratio) ** 0.5)\n",
    "            height = int((size / aspect_ratio) ** 0.5)\n",
    "            anchor_boxes.append((width, height))\n",
    "    return torch.tensor(anchor_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # We use tqdm to display a simple progress bar, allowing us to observe the learning progression.\n",
    "from torchmetrics.detection import mean_ap\n",
    "\n",
    "def fit(\n",
    "  model: nn.Module,\n",
    "  num_epochs: int,\n",
    "  optimizer: torch.optim.Optimizer,\n",
    "  train_dataloader: DataLoader,\n",
    "  val_dataloader: DataLoader,\n",
    "  device: str,\n",
    "  print_rate: int = 100\n",
    "  ):\n",
    "    # TODO: figure out accuacy\n",
    "    #accuracy = torchmetrics.Accuracy(task='multiclass', average=\"weighted\").to(model.device)\n",
    "    accuracy = None\n",
    "    model = model.to(device=device)\n",
    "    box_loss = nn.MSELoss()\n",
    "    obj_loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Iterate through epochs with tqdm\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch: {epoch}\\n\")\n",
    "        train_loss = 0\n",
    "        model.train()  # Set model to train\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            pred_boxes = outputs[..., 1:]\n",
    "            pred_obj = outputs[..., 0]\n",
    "\n",
    "            true_boxes = y[..., 1:]\n",
    "            true_obj = y[..., 0]\n",
    "            \n",
    "             # localization loss\n",
    "            box_loss_value = box_loss(pred_boxes, true_boxes)\n",
    "            \n",
    "            # objectness loss\n",
    "            obj_loss_value = obj_loss(pred_obj, true_obj)\n",
    "\n",
    "            # total loss\n",
    "            loss = box_loss_value + obj_loss_value\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % print_rate == 0: \n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{batch * len(X)}/{len(train_dataloader.dataset)} Samples\")\n",
    "            elif batch == len(train_dataloader) - 1:\n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{len(train_dataloader.dataset)}/{len(train_dataloader.dataset)} Samples\")\n",
    "        \n",
    "        # Divide the train_loss by the number of batches to get the average train_loss\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        # Setup the Val Loss and Accuracy to accumulate over the batches in the val dataset\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        ## Set model to evaluation mode and use torch.inference_mode to remove unnecessary training operations \n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X_val, y_val in val_dataloader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                # localization loss\n",
    "                box_loss_value = box_loss(pred_boxes, true_boxes)\n",
    "                # objectness loss\n",
    "                obj_loss_value = obj_loss(pred_obj, true_obj)\n",
    "                # total loss\n",
    "                loss = box_loss_value + obj_loss_value\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                #TODO: calculate accuracy\n",
    "#\n",
    "        ## Get the average Val Loss and Accuracy\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        avg_val_acc = val_acc / len(val_dataloader)\n",
    "#\n",
    "        print(f\"Train loss: {avg_train_loss} | Val Loss: {avg_val_loss} | Val Accuracy: {avg_val_acc}\")\n",
    "        if WANDB_LOGGING:\n",
    "            wandb.log({\"Train Loss\": avg_train_loss,\"Val Loss\": avg_val_loss, \"Val Accuracy\": avg_val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_utils import MTGCardsDataset, ImageTransformer\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "transformer = ImageTransformer()\n",
    "transform_pipe = transformer.get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "anchor_boxes = generate_anchor_boxes(sizes=CONFIG[\"dataset\"][\"anchor_box_sizes\"], aspect_ratios=CONFIG[\"dataset\"][\"anchor_box_aspect_ratios\"])\n",
    "\n",
    "model = CardDetector(img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "                      anchor_boxes=anchor_boxes,\n",
    "                        num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "                          loss_fn=nn.L1Loss(reduction=\"sum\")\n",
    "                          )\n",
    "\n",
    "dummy_image = torch.randn(1, 3, CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]) # create a dummy image to pass to the model and get feature map size\n",
    "feature_map_shape = model.feature_extractor(dummy_image).shape\n",
    "feature_map_dims = (feature_map_shape[-2], feature_map_shape[-1])\n",
    "\n",
    "\n",
    "train_dataset = MTGCardsDataset(annotations_file=CONFIG[\"dataset\"][\"annotations_file\"],\n",
    "                                img_dir=CONFIG[\"dataset\"][\"img_dir\"],\n",
    "                                  anchor_boxes=model.anchor_boxes,\n",
    "                                    feature_map_dims=feature_map_dims,\n",
    "                                      img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "                                        num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "                                          num_max_boxes=1,\n",
    "                                            transform=transform_pipe\n",
    "                                            )\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(annotations_file=CONFIG[\"dataset\"][\"annotations_file\"],\n",
    "                                img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "                                  anchor_boxes=model.anchor_boxes,\n",
    "                                    feature_map_dims=feature_map_dims,\n",
    "                                      img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "                                        num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "                                          num_max_boxes=1,\n",
    "                                            transform=transform_pipe\n",
    "                                            )\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "fit(model=model, num_epochs=50, optimizer=opt, train_dataloader= train_dataloader, val_dataloader= val_dataloader, device= (\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "test_image = read_image(\"data/images/image_01.jpg\")\n",
    "test_transform_pipe = transformer.get_test_transform_pipe(640, 640)\n",
    "model_input = test_transform_pipe(test_image)\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "pred_box = model.predict(model_input)#, ground_truth=feature_label.unsqueeze(0))\n",
    "print(pred_box.shape)\n",
    "\n",
    "#test_image = fn.convert_image_dtype(image=test_image, dtype=torch.uint8)\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=pred_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
