{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from card_detector import CardDetector, fit\n",
    "from dataloader_utils import MTGCardsDataset, get_transform_pipe\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = torch.Tensor([[198.27963804, 206.74086672],\n",
    "       [129.59395666, 161.90171490],\n",
    "       [161.65437828, 232.34624509]\n",
    "]) # Anchor boxes acquired from k-means clustering of the dataset\n",
    "\n",
    "model = CardDetector(\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  anchor_boxes=anchor_boxes,\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    ")\n",
    "\n",
    "feature_map_dims = (model.features_w, model.features_h)\n",
    "\n",
    "transform_pipe = get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "train_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_train\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_val\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "fit(\n",
    "    model=model,\n",
    "    num_epochs=CONFIG[\"optimizer\"][\"num_epochs\"],\n",
    "    optimizer=opt, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as fn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "images, labels = next(iter(val_dataloader))\n",
    "\n",
    "# Load the unaltered (not normalized) version of the image\n",
    "test_image = read_image(\"data/aug_val/1060_2.png\", mode=ImageReadMode.RGB)\n",
    "\n",
    "# Predict the bounding box and get the true box\n",
    "model = model.to(\"cpu\")\n",
    "pred_box = model.predict(images[1]).squeeze(0)\n",
    "true_box = model.predict(images[1], ground_truth=labels[1].unsqueeze(0)).squeeze(0)\n",
    "\n",
    "print(pred_box)\n",
    "print(true_box)\n",
    "\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=pred_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=true_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.to_pil_image(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "test_array = np.array(test_image).transpose(1, 2, 0)\n",
    "position = [442.5,269.0,95,128]\n",
    "offsetx = int(position[0] - position[2]/2)\n",
    "offsety = int(position[1] - position[3]/2)\n",
    "width = position[2]\n",
    "height = position[3]\n",
    "\n",
    "test_array[offsety:offsety + height, offsetx:offsetx+1] = 255\n",
    "test_array[offsety:offsety + height, offsetx + width:offsetx + width + 1] = 255\n",
    "\n",
    "test_array[offsety:offsety + 1, offsetx:offsetx + width] = 255\n",
    "test_array[offsety + height:offsety + height + 1, offsetx:offsetx + width] = 255\n",
    "\n",
    "test_bg = transforms.ToPILImage()(test_array)\n",
    "test_bg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
