{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_convert\n",
    "from source_code.card_detector import CardDetector, fit\n",
    "from source_code.dataloader_utils import MTGCardsDataset, get_transform_pipe\n",
    "from source_code.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = torch.Tensor([[198.27963804, 206.74086672],\n",
    "       [129.59395666, 161.90171490],\n",
    "       [161.65437828, 232.34624509]\n",
    "]) # Anchor boxes acquired from k-means clustering of the dataset\n",
    "\n",
    "model = CardDetector(\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  anchor_boxes=anchor_boxes,\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    ")\n",
    "\n",
    "feature_map_dims = (model.features_w, model.features_h)\n",
    "\n",
    "transform_pipe = get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "train_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_train\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_val\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) \n",
    "\n",
    "test_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_test\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "fit(\n",
    "    model=model,\n",
    "    num_epochs=CONFIG[\"optimizer\"][\"num_epochs\"],\n",
    "    optimizer=opt, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as fn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "images, labels = next(iter(test_dataloader))\n",
    "\n",
    "# Load the unaltered (not normalized) version of the image\n",
    "test_image = read_image(\"./data/aug_test/10621_1.png\", mode=ImageReadMode.RGB)\n",
    "index = 20\n",
    "\n",
    "# Predict the bounding box and get the true box\n",
    "model = model.to(device=model.device)\n",
    "\n",
    "image = images[index]\n",
    "pred_boxes = model.predict(image, keep_box_score_treshhold=0, num_max_boxes=1)\n",
    "pred_box = pred_boxes.squeeze(0)\n",
    "\n",
    "true_boxes = model.predict(image, ground_truth=labels[index].unsqueeze(0))\n",
    "true_box = true_boxes.squeeze(0)[:1]\n",
    "\n",
    "print(\"pred:\", pred_box)\n",
    "print(\"true:\",true_box)\n",
    "\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=pred_box, width=3, colors=(255,255,0))\n",
    "\n",
    "box_conv = box_convert(pred_box, in_fmt='xyxy', out_fmt='cxcywh')\n",
    "location_cell = torch.Tensor([[box_conv[0], box_conv[1], box_conv[0]+32, box_conv[1]+32]])\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=location_cell, width=3, colors=(255,255,0))\n",
    "\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=true_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "position = [175.5,354.0,107,256]\n",
    "\n",
    "anchors_array = np.array(anchor_boxes)\n",
    "anchor_boxes_to_draw = torch.Tensor([[position[0] - anchor[0]/2, position[1] - anchor[1]/2, position[0] + anchor[0]/2, position[1] + anchor[1]/2] for anchor in anchors_array])\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=anchor_boxes_to_draw, width=3, colors=(150,150,150))\n",
    "\n",
    "box = [position[0] - position[2]/2, position[1] - position[3]/2, position[0] + position[2]/2, position[1] + position[3]/2]\n",
    "boxes = torch.Tensor([box])\n",
    "bbox_img_tensor = draw_bounding_boxes(image=bbox_img_tensor, boxes=boxes, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
