{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_convert\n",
    "from source_code.card_detector import CardDetector, fit\n",
    "from source_code.dataloader_utils import MTGCardsDataset, get_transform_pipe\n",
    "from source_code.config import *\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = CONFIG[\"training\"][\"SOURCE_PATH\"]\n",
    "DEST_PATH = CONFIG[\"training\"][\"DEST_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = torch.Tensor(CONFIG[\"training\"][\"ANCHOR_BOXES\"]) # Anchor boxes acquired from k-means clustering of the dataset\n",
    "\n",
    "model = CardDetector(\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  anchor_boxes=anchor_boxes,\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  head='v3'\n",
    ")\n",
    "if SOURCE_PATH is not None:\n",
    "    model.load_state_dict(torch.load(SOURCE_PATH))\n",
    "\n",
    "feature_map_dims = (model.features_w, model.features_h)\n",
    "\n",
    "transform_pipe = get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "train_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_train\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_val\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) \n",
    "\n",
    "test_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_test\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikioza\u001b[0m (\u001b[33mfiit-nn-2023-lp-vs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\VScode Projects\\FIIT_MASTERS\\NSIETE\\nsiete_projects\\NN_assignment_3\\wandb\\run-20230504_154632-n8hkl12p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fiit-nn-2023-lp-vs/card_detector/runs/n8hkl12p' target=\"_blank\">elegant-nerf-herder-9</a></strong> to <a href='https://wandb.ai/fiit-nn-2023-lp-vs/card_detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fiit-nn-2023-lp-vs/card_detector' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/card_detector</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fiit-nn-2023-lp-vs/card_detector/runs/n8hkl12p' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/card_detector/runs/n8hkl12p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564d2c64fc104a6d8a6a88d0018ed496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikio\\anaconda3\\envs\\nsiete_pytorch_project\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    1.5068932995200157\n",
      "Train Objectness Loss:   0.5795831400901079\n",
      "Train Localization Loss: 0.9273101650178432\n",
      "Val Total Loss:          1.0573624297976494\n",
      "Val Objectness Loss:     0.5230606980621815\n",
      "Val Localization Loss:   0.5343017242848873\n",
      "\n",
      "Epoch: 1\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.7888402044773102\n",
      "Train Objectness Loss:   0.42701535671949387\n",
      "Train Localization Loss: 0.3618248403072357\n",
      "Val Total Loss:          0.7199097685515881\n",
      "Val Objectness Loss:     0.3478526957333088\n",
      "Val Localization Loss:   0.3720570709556341\n",
      "\n",
      "Epoch: 2\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.6189899332821369\n",
      "Train Objectness Loss:   0.33190469816327095\n",
      "Train Localization Loss: 0.2870852369815111\n",
      "Val Total Loss:          0.586013201624155\n",
      "Val Objectness Loss:     0.2795162443071604\n",
      "Val Localization Loss:   0.3064969554543495\n",
      "\n",
      "Epoch: 3\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.534597409889102\n",
      "Train Objectness Loss:   0.2697120700031519\n",
      "Train Localization Loss: 0.26488534826785326\n",
      "Val Total Loss:          0.5278853736817837\n",
      "Val Objectness Loss:     0.24327957443892956\n",
      "Val Localization Loss:   0.2846057917922735\n",
      "\n",
      "Epoch: 4\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.48169839940965176\n",
      "Train Objectness Loss:   0.22769917733967304\n",
      "Train Localization Loss: 0.25399922486394644\n",
      "Val Total Loss:          0.48820274136960506\n",
      "Val Objectness Loss:     0.21327271405607462\n",
      "Val Localization Loss:   0.274930028244853\n",
      "\n",
      "Epoch: 5\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.4453484769910574\n",
      "Train Objectness Loss:   0.19845288898795843\n",
      "Train Localization Loss: 0.2468955833464861\n",
      "Val Total Loss:          0.4587282743304968\n",
      "Val Objectness Loss:     0.18943125568330288\n",
      "Val Localization Loss:   0.2692970186471939\n",
      "\n",
      "Epoch: 6\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.41907115653157234\n",
      "Train Objectness Loss:   0.17756971344351768\n",
      "Train Localization Loss: 0.2415014375001192\n",
      "Val Total Loss:          0.4371224157512188\n",
      "Val Objectness Loss:     0.1716954344883561\n",
      "Val Localization Loss:   0.26542697940021753\n",
      "\n",
      "Epoch: 7\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3993193805217743\n",
      "Train Objectness Loss:   0.16228541731834412\n",
      "Train Localization Loss: 0.23703396506607533\n",
      "Val Total Loss:          0.4211129769682884\n",
      "Val Objectness Loss:     0.15862608887255192\n",
      "Val Localization Loss:   0.26248688343912363\n",
      "\n",
      "Epoch: 8\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.38396764174103737\n",
      "Train Objectness Loss:   0.15080515667796135\n",
      "Train Localization Loss: 0.23316248878836632\n",
      "Val Total Loss:          0.40878950245678425\n",
      "Val Objectness Loss:     0.14859120920300484\n",
      "Val Localization Loss:   0.26019829511642456\n",
      "\n",
      "Epoch: 9\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3716693799942732\n",
      "Train Objectness Loss:   0.14195326808840036\n",
      "Train Localization Loss: 0.22971611004322767\n",
      "Val Total Loss:          0.3991972003132105\n",
      "Val Objectness Loss:     0.1408309955149889\n",
      "Val Localization Loss:   0.25836620572954416\n",
      "\n",
      "Epoch: 10\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.36152726598083973\n",
      "Train Objectness Loss:   0.13495087064802647\n",
      "Train Localization Loss: 0.226576398126781\n",
      "Val Total Loss:          0.39162259735167027\n",
      "Val Objectness Loss:     0.13470990397036076\n",
      "Val Localization Loss:   0.25691269151866436\n",
      "\n",
      "Epoch: 11\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3529326878488064\n",
      "Train Objectness Loss:   0.129273503087461\n",
      "Train Localization Loss: 0.2236591810360551\n",
      "Val Total Loss:          0.3855053223669529\n",
      "Val Objectness Loss:     0.12973263021558523\n",
      "Val Localization Loss:   0.25577269308269024\n",
      "\n",
      "Epoch: 12\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3454557377845049\n",
      "Train Objectness Loss:   0.12455718452110887\n",
      "Train Localization Loss: 0.2208985546603799\n",
      "Val Total Loss:          0.38055034168064594\n",
      "Val Objectness Loss:     0.12564691761508584\n",
      "Val Localization Loss:   0.25490342173725367\n",
      "\n",
      "Epoch: 13\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3387962281703949\n",
      "Train Objectness Loss:   0.12054886529222131\n",
      "Train Localization Loss: 0.21824736334383488\n",
      "Val Total Loss:          0.37649464048445225\n",
      "Val Objectness Loss:     0.12221484817564487\n",
      "Val Localization Loss:   0.25427978951483965\n",
      "\n",
      "Epoch: 14\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3327470887452364\n",
      "Train Objectness Loss:   0.11706579755991697\n",
      "Train Localization Loss: 0.21568129118531942\n",
      "Val Total Loss:          0.3731476850807667\n",
      "Val Objectness Loss:     0.1192904687486589\n",
      "Val Localization Loss:   0.25385721772909164\n",
      "\n",
      "Epoch: 15\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3271437641233206\n",
      "Train Objectness Loss:   0.11397709418088198\n",
      "Train Localization Loss: 0.2131666699424386\n",
      "Val Total Loss:          0.37038957700133324\n",
      "Val Objectness Loss:     0.11674523632973433\n",
      "Val Localization Loss:   0.25364434253424406\n",
      "\n",
      "Epoch: 16\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3218634631484747\n",
      "Train Objectness Loss:   0.11118191620334983\n",
      "Train Localization Loss: 0.210681545548141\n",
      "Val Total Loss:          0.36812201514840126\n",
      "Val Objectness Loss:     0.11449474794790149\n",
      "Val Localization Loss:   0.2536272658035159\n",
      "\n",
      "Epoch: 17\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3168273586779833\n",
      "Train Objectness Loss:   0.10861223051324487\n",
      "Train Localization Loss: 0.20821513049304485\n",
      "Val Total Loss:          0.3662411868572235\n",
      "Val Objectness Loss:     0.11246490990743041\n",
      "Val Localization Loss:   0.25377627834677696\n",
      "\n",
      "Epoch: 18\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3119718413800001\n",
      "Train Objectness Loss:   0.10622602934017777\n",
      "Train Localization Loss: 0.2057458097115159\n",
      "Val Total Loss:          0.3646897468715906\n",
      "Val Objectness Loss:     0.11061285249888897\n",
      "Val Localization Loss:   0.2540768990293145\n",
      "\n",
      "Epoch: 19\n",
      "Looked at 16 Batches\t---\t1000/1000 Samples\n",
      "Train Total Loss:    0.3072308599948883\n",
      "Train Objectness Loss:   0.10397678799927235\n",
      "Train Localization Loss: 0.2032540701329708\n",
      "Val Total Loss:          0.36348889395594597\n",
      "Val Objectness Loss:     0.10891074314713478\n",
      "Val Localization Loss:   0.25457815174013376\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Localization Loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Objectness Loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Total Loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Localization Loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Objectness Loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Total Loss</td><td>█▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Localization Loss</td><td>0.20325</td></tr><tr><td>Train Objectness Loss</td><td>0.10398</td></tr><tr><td>Train Total Loss</td><td>0.30723</td></tr><tr><td>Val Localization Loss</td><td>0.25458</td></tr><tr><td>Val Objectness Loss</td><td>0.10891</td></tr><tr><td>Val Total Loss</td><td>0.36349</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elegant-nerf-herder-9</strong> at: <a href='https://wandb.ai/fiit-nn-2023-lp-vs/card_detector/runs/n8hkl12p' target=\"_blank\">https://wandb.ai/fiit-nn-2023-lp-vs/card_detector/runs/n8hkl12p</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230504_154632-n8hkl12p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if WANDB_LOGGING:\n",
    "    wandb.init(project=CONFIG['project_name'])\n",
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "fit(\n",
    "    model=model,\n",
    "    num_epochs=CONFIG[\"optimizer\"][\"num_epochs\"],\n",
    "    optimizer=opt, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=model.device,\n",
    "    localization_weight=CONFIG[\"training\"][\"LOCALIZATION_WEIGHT\"],\n",
    "    save_path=DEST_PATH\n",
    ")\n",
    "if WANDB_LOGGING:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
