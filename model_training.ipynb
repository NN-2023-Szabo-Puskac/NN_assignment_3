{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from source_code.card_detector import CardDetector, fit\n",
    "from source_code.dataloader_utils import MTGCardsDataset, get_transform_pipe\n",
    "from source_code.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = torch.Tensor([[198.27963804, 206.74086672],\n",
    "       [129.59395666, 161.90171490],\n",
    "       [161.65437828, 232.34624509]\n",
    "]) # Anchor boxes acquired from k-means clustering of the dataset\n",
    "\n",
    "model = CardDetector(\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  anchor_boxes=anchor_boxes,\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    ")\n",
    "\n",
    "feature_map_dims = (model.features_w, model.features_h)\n",
    "\n",
    "transform_pipe = get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "train_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_train\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_val\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) \n",
    "\n",
    "test_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file_test\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=CONFIG[\"dataset\"][\"num_max_boxes\"],\n",
    "  transform=transform_pipe,\n",
    "  limit=CONFIG[\"dataset\"][\"limit\"]\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "# fit(\n",
    "#     model=model,\n",
    "#     num_epochs=CONFIG[\"optimizer\"][\"num_epochs\"],\n",
    "#     optimizer=opt, \n",
    "#     train_dataloader=train_dataloader,\n",
    "#     val_dataloader=val_dataloader,\n",
    "#     device=model.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikio\\anaconda3\\envs\\nsiete_pytorch_project\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "kept_objectness_scores: tensor([0.6054, 0.6048, 0.6040, 0.6019, 0.6004])\n",
      "kept_boxes: tensor([[244.5706, 267.2888, 380.4504, 402.6197],\n",
      "        [139.3698, 222.1493, 299.4608, 375.0262],\n",
      "        [394.7342, 409.2305, 546.4748, 685.9588],\n",
      "        [343.5951, 437.5288, 484.2244, 712.8331],\n",
      "        [239.2680, 150.6956, 382.4750, 320.0624]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m pred_boxes \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(image, keep_box_score_treshhold\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m pred_box \u001b[39m=\u001b[39m pred_boxes\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m true_boxes \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(image, ground_truth\u001b[39m=\u001b[39;49mlabels[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     19\u001b[0m true_box \u001b[39m=\u001b[39m true_boxes\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)[:\u001b[39m1\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpred:\u001b[39m\u001b[39m\"\u001b[39m, pred_box)\n",
      "File \u001b[1;32mc:\\VScode Projects\\FIIT_MASTERS\\NSIETE\\nsiete_projects\\NN_assignment_3\\source_code\\card_detector.py:131\u001b[0m, in \u001b[0;36mCardDetector.predict\u001b[1;34m(self, input, keep_box_score_treshhold, num_max_boxes, ground_truth)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     detection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39minput\u001b[39m)\n\u001b[1;32m--> 131\u001b[0m detection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_prediction_to_input_shape(detection)\n\u001b[0;32m    133\u001b[0m \u001b[39m# Apply sigmoid to the objectness scores\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39mif\u001b[39;00m ground_truth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\VScode Projects\\FIIT_MASTERS\\NSIETE\\nsiete_projects\\NN_assignment_3\\source_code\\card_detector.py:184\u001b[0m, in \u001b[0;36mCardDetector.scale_prediction_to_input_shape\u001b[1;34m(self, detection)\u001b[0m\n\u001b[0;32m    177\u001b[0m anchor_box_offsets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_anchor_box_offsets(\n\u001b[0;32m    178\u001b[0m     detection_shape\u001b[39m=\u001b[39mdetection\u001b[39m.\u001b[39mshape, scale_w\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_w, scale_h\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_h\n\u001b[0;32m    179\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    181\u001b[0m detection[:, :, :, :, \u001b[39m3\u001b[39m:FEATURES_IN_ANCHOR] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\n\u001b[0;32m    182\u001b[0m     detection[:, :, :, :, \u001b[39m3\u001b[39m:FEATURES_IN_ANCHOR]\n\u001b[0;32m    183\u001b[0m )\n\u001b[1;32m--> 184\u001b[0m detection[:, :, :, :, \u001b[39m3\u001b[39m:FEATURES_IN_ANCHOR] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmul(\n\u001b[0;32m    185\u001b[0m     detection[:, :, :, :, \u001b[39m3\u001b[39;49m:FEATURES_IN_ANCHOR],\n\u001b[0;32m    186\u001b[0m     anchor_box_scales[:, :, :, :, \u001b[39m3\u001b[39;49m:FEATURES_IN_ANCHOR],\n\u001b[0;32m    187\u001b[0m )  \u001b[39m# multiply the w, h coords of detection with predifined anchor box w, h\u001b[39;00m\n\u001b[0;32m    188\u001b[0m detection[:, :, :, :, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(\n\u001b[0;32m    189\u001b[0m     detection[:, :, :, :, \u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_w\n\u001b[0;32m    190\u001b[0m )  \u001b[39m# scale the x offset from cell origin\u001b[39;00m\n\u001b[0;32m    191\u001b[0m detection[:, :, :, :, \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(\n\u001b[0;32m    192\u001b[0m     detection[:, :, :, :, \u001b[39m2\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_h\n\u001b[0;32m    193\u001b[0m )  \u001b[39m# scale the y offset from cell origin\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.functional as fn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "images, labels = next(iter(test_dataloader))\n",
    "\n",
    "# Load the unaltered (not normalized) version of the image\n",
    "test_image = read_image(\"data/aug_test/10618_1.png\", mode=ImageReadMode.RGB)\n",
    "\n",
    "# Predict the bounding box and get the true box\n",
    "model = model.to(device=model.device)\n",
    "\n",
    "image = images[0]\n",
    "pred_boxes = model.predict(image, keep_box_score_treshhold=0)\n",
    "pred_box = pred_boxes.squeeze(0)\n",
    "\n",
    "true_boxes = model.predict(image, ground_truth=labels[1].unsqueeze(0))\n",
    "true_box = true_boxes.squeeze(0)[:1]\n",
    "\n",
    "print(\"pred:\", pred_box)\n",
    "print(\"true:\",true_box)\n",
    "\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=pred_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=true_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.to_pil_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "position = [291.5,321.5,135,203]\n",
    "\n",
    "anchors_array = np.array(anchor_boxes)\n",
    "anchor_boxes_to_draw = torch.Tensor([[position[0] - anchor[0]/2, position[1] - anchor[1]/2, position[0] + anchor[0]/2, position[1] + anchor[1]/2] for anchor in anchors_array])\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=anchor_boxes_to_draw, width=3, colors=(150,150,150))\n",
    "\n",
    "box = [position[0] - position[2]/2, position[1] - position[3]/2, position[0] + position[2]/2, position[1] + position[3]/2]\n",
    "boxes = torch.Tensor([box])\n",
    "bbox_img_tensor = draw_bounding_boxes(image=bbox_img_tensor, boxes=boxes, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
