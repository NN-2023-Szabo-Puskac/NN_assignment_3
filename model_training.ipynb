{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from card_detector import CardDetector, fit\n",
    "from dataloader_utils import MTGCardsDataset, get_transform_pipe\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikio\\anaconda3\\envs\\nsiete_pytorch_project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vikio\\anaconda3\\envs\\nsiete_pytorch_project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "anchor_boxes = torch.Tensor([[198.27963804, 206.74086672],\n",
    "       [129.59395666, 161.90171490],\n",
    "       [161.65437828, 232.34624509]\n",
    "]) # Anchor boxes acquired from k-means clustering of the dataset\n",
    "\n",
    "model = CardDetector(img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "                      anchor_boxes=anchor_boxes,\n",
    "                        num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "                          loss_fn=nn.L1Loss(reduction=\"sum\")\n",
    "                          )\n",
    "\n",
    "dummy_image = torch.randn(1, 3, CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]) # create a dummy image to pass to the model and get feature map size\n",
    "feature_map_shape = model.feature_extractor(dummy_image).shape\n",
    "feature_map_dims = (feature_map_shape[-2], feature_map_shape[-1])\n",
    "\n",
    "\n",
    "transform_pipe = get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "train_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=10_000\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=1000\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b959b858cc42a7aa7130669753effd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikio\\anaconda3\\envs\\nsiete_pytorch_project\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0 Batches\t---\t0/10000 Samples\n",
      "Looked at 100 Batches\t---\t3200/10000 Samples\n",
      "Looked at 200 Batches\t---\t6400/10000 Samples\n",
      "Looked at 300 Batches\t---\t9600/10000 Samples\n",
      "Looked at 312 Batches\t---\t10000/10000 Samples\n",
      "Train loss: 0.08157331809496728 | Val Loss: 0.012037569656968117 | Val Accuracy: 0.0\n",
      "Epoch: 1\n",
      "\n",
      "Looked at 0 Batches\t---\t0/10000 Samples\n",
      "Looked at 100 Batches\t---\t3200/10000 Samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mCONFIG[\u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m fit(model\u001b[39m=\u001b[39;49mmodel, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49mopt, train_dataloader\u001b[39m=\u001b[39;49m train_dataloader, val_dataloader\u001b[39m=\u001b[39;49m val_dataloader, device\u001b[39m=\u001b[39;49m (\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\VScode Projects\\FIIT_MASTERS\\NSIETE\\nsiete_projects\\NN_assignment_3\\card_detector.py:178\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, num_epochs, optimizer, train_dataloader, val_dataloader, device, print_rate)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39m# total loss\u001b[39;00m\n\u001b[0;32m    177\u001b[0m loss \u001b[39m=\u001b[39m box_loss_value \u001b[39m+\u001b[39m obj_loss_value\n\u001b[1;32m--> 178\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    180\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    181\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "fit(model=model, num_epochs=5, optimizer=opt, train_dataloader= train_dataloader, val_dataloader= val_dataloader, device= (\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as fn\n",
    "\n",
    "test_image = read_image(\"./data/augmented_1/991_3.png\", mode=ImageReadMode.RGB)\n",
    "test_transform_pipe = get_transform_pipe(640, 640)\n",
    "model_input = test_transform_pipe(test_image)\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "pred_box = model.predict(model_input)#, ground_truth=feature_label.unsqueeze(0))\n",
    "pred_box = pred_box.squeeze(0)\n",
    "print(pred_box)\n",
    "\n",
    "#test_image = fn.convert_image_dtype(image=test_image, dtype=torch.uint8)\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=pred_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
