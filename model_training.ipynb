{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from card_detector import CardDetector, fit\n",
    "from dataloader_utils import MTGCardsDataset, get_transform_pipe\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = torch.Tensor([[198.27963804, 206.74086672],\n",
    "       [129.59395666, 161.90171490],\n",
    "       [161.65437828, 232.34624509]\n",
    "]) # Anchor boxes acquired from k-means clustering of the dataset\n",
    "\n",
    "model = CardDetector(img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "                      anchor_boxes=anchor_boxes,\n",
    "                        num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "                          loss_fn=nn.L1Loss(reduction=\"sum\")\n",
    "                          )\n",
    "\n",
    "dummy_image = torch.randn(1, 3, CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]) # create a dummy image to pass to the model and get feature map size\n",
    "feature_map_shape = model.feature_extractor(dummy_image).shape\n",
    "feature_map_dims = (feature_map_shape[-2], feature_map_shape[-1])\n",
    "\n",
    "\n",
    "transform_pipe = get_transform_pipe(img_w=CONFIG[\"dataset\"][\"img_w\"], img_h=CONFIG[\"dataset\"][\"img_h\"])\n",
    "\n",
    "train_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=13\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"])\n",
    "\n",
    "val_dataset = MTGCardsDataset(\n",
    "  annotations_file=CONFIG[\"dataset\"][\"annotations_file\"],\n",
    "  img_dir=CONFIG[\"dataset\"][\"img_dir\"], #TODO: change directory when we have the actual data\n",
    "  anchor_boxes=model.anchor_boxes,\n",
    "  feature_map_dims=feature_map_dims,\n",
    "  img_dims= (CONFIG[\"dataset\"][\"img_w\"], CONFIG[\"dataset\"][\"img_h\"]),\n",
    "  num_anchors_per_cell=CONFIG[\"dataset\"][\"num_anchors_per_cell\"],\n",
    "  num_max_boxes=1,\n",
    "  transform=transform_pipe,\n",
    "  limit=13\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params=model.parameters(), lr=CONFIG[\"optimizer\"][\"lr\"])\n",
    "fit(model=model, num_epochs=30, optimizer=opt, train_dataloader= train_dataloader, val_dataloader= val_dataloader, device= (\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as fn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "test_image = read_image(\"data/images_2/1_2.png\", mode=ImageReadMode.RGB)\n",
    "test_transform_pipe = get_transform_pipe(640, 640)\n",
    "model_input = test_transform_pipe(test_image)\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "pred_box = model.predict(model_input)#, ground_truth=feature_label.unsqueeze(0))\n",
    "pred_box = pred_box.squeeze(0)\n",
    "print(pred_box)\n",
    "\n",
    "#test_image = fn.convert_image_dtype(image=test_image, dtype=torch.uint8)\n",
    "bbox_img_tensor = draw_bounding_boxes(image=test_image, boxes=pred_box, width=3, colors=(255,255,0))\n",
    "\n",
    "fn.to_pil_image(bbox_img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
